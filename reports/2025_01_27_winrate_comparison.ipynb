{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Evaluations for Multiple Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import DataFrame, json_normalize\n",
    "from typing import List, Dict, Any, Tuple, Union, Optional\n",
    "\n",
    "LOGS_PATH: str = \"../expt-logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils import load_agent_logs_df, read_jsonl_as_json, load_game_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPT_NAMES: List[str] = [\n",
    "    \"2025-01-25_phi_llama_100_games\",\n",
    "    \"2025-01-27_llama_phi_100_games\",\n",
    "    \"2025-01-28_phi_phi_100_games\",\n",
    "    \"2025-01-28_llama_llama_100_games\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCRIPTIONS: List[str] = [\n",
    "    \"Crewmate: Phi 4, Impostor: Llama 3.3\",\n",
    "    \"Crewmate: Llama 3.3, Impostor: Phi 4\",\n",
    "    \"Crewmate: Phi 4, Impostor: Phi 4\",\n",
    "    \"Crewmate: Llama 3.3, Impostor: Llama 3.3\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_logs_paths: List[str] = [\n",
    "    os.path.join(LOGS_PATH, expt_name, \"summary.json\") for expt_name in EXPT_NAMES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dfs: List[DataFrame] = [\n",
    "    load_game_summary(logs_path) for logs_path in summary_logs_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons = [\n",
    "    \"Impostors Win! (outnumbered C.)\",\n",
    "    \"Crewmates Win! (tasks done)\",\n",
    "    \"Crewmates Win! (voted out I.)\",\n",
    "    \"Impostors Win! (time up)\",\n",
    "]\n",
    "\n",
    "for i, df in enumerate(summary_dfs):\n",
    "    df['Winner Reason'] = df['Winner'].apply(lambda x: reasons[x-1])\n",
    "    df['Models'] = DESCRIPTIONS[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df: DataFrame = pd.concat(summary_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Function to calculate bootstrap confidence intervals\n",
    "def calculate_bootstrap_ci(data, n_bootstrap=1000, ci=0.90):\n",
    "    \"\"\"\n",
    "    Calculate bootstrap confidence intervals for a binary dataset.\n",
    "    \n",
    "    Args:\n",
    "        data: Binary data array\n",
    "        n_bootstrap: Number of bootstrap samples\n",
    "        ci: Confidence interval (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (lower bound, upper bound)\n",
    "    \"\"\"\n",
    "    bootstrap_samples = np.random.choice(data, size=(n_bootstrap, len(data)), replace=True)\n",
    "    bootstrap_counts = np.sum(bootstrap_samples, axis=1)\n",
    "    lower_percentile = (1 - ci) / 2 * 100\n",
    "    upper_percentile = (1 + ci) / 2 * 100\n",
    "    lower = np.percentile(bootstrap_counts, lower_percentile)\n",
    "    upper = np.percentile(bootstrap_counts, upper_percentile)\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_winner_reasons_with_ci(combined_df, n_bootstrap=1000, ci=0.90):\n",
    "    \"\"\"\n",
    "    Plot winner reasons with bootstrap confidence intervals.\n",
    "    \n",
    "    Args:\n",
    "        combined_df: DataFrame with game results\n",
    "        n_bootstrap: Number of bootstrap samples\n",
    "        ci: Confidence interval (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure: The figure object\n",
    "    \"\"\"\n",
    "    # Softer color theme\n",
    "    custom_colors = [\n",
    "    (\"#c97b63\", \"#934d30\"),  # Soft clay + burnt sienna\n",
    "    (\"#6e7c70\", \"#3f4d46\"),  # Muted moss + charcoal green\n",
    "    (\"#7a9e76\", \"#486b49\"),  # Dusty sage + forest moss\n",
    "    (\"#b29fce\", \"#77619e\"),  # Dusty lilac + dusky violet\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Get unique winner reasons and models\n",
    "    winner_reasons = combined_df['Winner Reason'].unique()\n",
    "    models = combined_df['Models'].unique()\n",
    "    \n",
    "    # Create wrapped text labels for x-ticks\n",
    "    wrapped_labels = []\n",
    "    for reason in winner_reasons:\n",
    "        # Split long reasons at appropriate points and join with <br> for line breaks\n",
    "        words = reason.split()\n",
    "        if len(words) > 3:\n",
    "            # Split roughly in half\n",
    "            mid = len(words) // 2\n",
    "            wrapped_labels.append(' '.join(words[:mid]) + '<br>' + ' '.join(words[mid:]))\n",
    "        else:\n",
    "            wrapped_labels.append(reason)\n",
    "\n",
    "    # Create a figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Calculate bar positions\n",
    "    num_reasons = len(winner_reasons)\n",
    "    num_models = len(models)\n",
    "    bar_width = 0.8 / num_models  # Width of each bar\n",
    "    bar_positions = {}\n",
    "    \n",
    "    for i, reason in enumerate(winner_reasons):\n",
    "        for j, model in enumerate(models):\n",
    "            # Calculate position for this bar\n",
    "            position = i + (j - num_models/2 + 0.5) * bar_width\n",
    "            if reason not in bar_positions:\n",
    "                bar_positions[reason] = {}\n",
    "            bar_positions[reason][model] = position\n",
    "\n",
    "    # For each model, add a bar trace with error bars\n",
    "    for j, model in enumerate(models):\n",
    "        model_data = combined_df[combined_df['Models'] == model]\n",
    "        \n",
    "        counts = []\n",
    "        error_y = []\n",
    "        positions = []\n",
    "        \n",
    "        for reason in winner_reasons:\n",
    "            # Count occurrences of this reason for this model\n",
    "            reason_data = (model_data['Winner Reason'] == reason).astype(int)\n",
    "            count = reason_data.sum()\n",
    "            counts.append(count)\n",
    "            positions.append(bar_positions[reason][model])\n",
    "            \n",
    "            # Calculate bootstrap confidence intervals\n",
    "            lower, upper = calculate_bootstrap_ci(reason_data.values, n_bootstrap, ci)\n",
    "            error_y.append((upper - lower) / 2)  # Use average of upper and lower as error\n",
    "        \n",
    "        # Add bar trace with error bars using Among Us colors\n",
    "        color_idx = j % len(custom_colors)\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=positions,  # Use custom positions\n",
    "            y=counts,\n",
    "            name=model,\n",
    "            marker_color=custom_colors[color_idx][0],\n",
    "            marker_line_color=custom_colors[color_idx][1],\n",
    "            marker_line_width=1.2,\n",
    "            error_y=dict(\n",
    "                type='data',\n",
    "                array=error_y,\n",
    "                visible=True,\n",
    "                color=custom_colors[color_idx][1]\n",
    "            ),\n",
    "            width=bar_width * 0.9  # Slightly narrower than calculated width\n",
    "        ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"\",\n",
    "        xaxis_title=\"Winner (Reason)\",\n",
    "        yaxis_title=\"Number of Games\",\n",
    "        barmode=\"group\",\n",
    "        showlegend=True,\n",
    "        plot_bgcolor='white'\n",
    "    )\n",
    "    \n",
    "    # Set custom tick positions and labels\n",
    "    fig.update_xaxes(\n",
    "        tickmode='array',\n",
    "        tickvals=list(range(len(winner_reasons))),  # Center positions for each group\n",
    "        ticktext=wrapped_labels,\n",
    "        tickangle=0\n",
    "    )\n",
    "\n",
    "    # Ensure y-axis starts at zero\n",
    "    fig.update_yaxes(range=[0, max([max(counts) for counts in [\n",
    "        [reason_data.sum() for reason_data in \n",
    "         [(combined_df[combined_df['Models'] == model]['Winner Reason'] == reason).astype(int) \n",
    "          for reason in winner_reasons]]\n",
    "        for model in models\n",
    "    ]]) * 1.4])  # Add some headroom for error bars\n",
    "\n",
    "    # Legend settings with smaller size and reduced spacing\n",
    "    fig.update_layout(legend=dict(\n",
    "        orientation=\"v\",\n",
    "        x=0.25,\n",
    "        y=0.85,\n",
    "        # traceorder=\"normal\",\n",
    "        bgcolor=\"#f0f0f0\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "        font=dict(size=12),  # Even smaller font size\n",
    "        itemsizing='constant',  # Make all legend items same size\n",
    "        itemwidth=30,  # Further reduce width of legend items\n",
    "        yanchor=\"top\",\n",
    "        tracegroupgap=0,  # Remove gap between legend items\n",
    "        entrywidth=50,  # Reduce width of legend box\n",
    "        entrywidthmode='fraction',\n",
    "        itemclick=False,  # Prevent toggling visibility on click\n",
    "        xref='paper',\n",
    "        yref='paper',\n",
    "        xanchor='left'  # Anchor to right side\n",
    "    ))\n",
    "\n",
    "    # Grid lines\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "\n",
    "    fig.update_layout(\n",
    "        template='plotly_white', \n",
    "        font=dict(family=\"Computer Modern\", size=10),\n",
    "        plot_bgcolor='#fafaf7'\n",
    "    )\n",
    "\n",
    "    # Width and height\n",
    "    fig.update_layout(width=600, height=400)\n",
    "\n",
    "    # Font settings for research paper with smaller sizes\n",
    "    fig.update_layout(font=dict(family='serif', size=12, color='black'))\n",
    "    fig.update_xaxes(title_font=dict(family='serif', size=12, color='black'))\n",
    "    fig.update_yaxes(title_font=dict(family='serif', size=12, color='black'))\n",
    "    fig.update_xaxes(tickfont=dict(family='serif', size=12, color='black'))\n",
    "    fig.update_yaxes(tickfont=dict(family='serif', size=12, color='black'))\n",
    "    fig.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=False)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display the figure\n",
    "fig = plot_winner_reasons_with_ci(combined_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the figure in high-res pdf\n",
    "fig.write_image(\"plots/winner_reasons_with_ci.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amongus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}