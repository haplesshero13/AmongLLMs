{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'lolmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch as t\n",
    "import importlib\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pandas import DataFrame, json_normalize\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../linear-probes/')\n",
    "from probes import LinearProbe\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "from evaluate_utils import evaluate_probe_on_activation_dataset\n",
    "from plots import plot_behavior_distribution, plot_roc_curves, add_roc_curves, print_metrics, plot_roc_curve_eval\n",
    "import probes\n",
    "from pprint import pprint as pp\n",
    "\n",
    "from probe_datasets import AmongUsDataset, TruthfulQADataset, DishonestQADataset, RepEngDataset, RolePlayingDataset, ApolloProbeDataset\n",
    "from configs import config_phi4, config_gpt2, config_llama3\n",
    "base_config = config_phi4\n",
    "amongus_expt_name: str = \"2025-02-01_phi_phi_100_games_v3\"\n",
    "# layers_to_work_on: List[int] = list(range(base_config[\"num_layers\"]))\n",
    "layers_to_work_on: List[int] = [20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import config_phi4, config_gpt2, config_llama3\n",
    "base_config = config_phi4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer: int = layers_to_work_on[0]\n",
    "probe_dataset_name: str = 'RepEngDataset'\n",
    "\n",
    "probe = LinearProbe(base_config[\"activation_size\"])\n",
    "        \n",
    "checkpoint_path = f'../linear-probes/checkpoints/{probe_dataset_name}_probe_{base_config[\"short_name\"]}_layer{layer}.pkl'\n",
    "\n",
    "with open(checkpoint_path, 'rb') as f:\n",
    "    probe.model = pickle.load(f).model\n",
    "    print(f'Loaded probe trained on {probe_dataset_name} for layer {layer}.')\n",
    "\n",
    "config = base_config.copy()\n",
    "config[\"layer\"] = str(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AmongUsDataset(config, model=None, tokenizer=None, device='cpu', expt_name=config['expt_name'], test_split=1)\n",
    "all_probe_outputs = []\n",
    "chunk_size: int = 500\n",
    "list_of_chunks_to_eval = [1]\n",
    "row_indices = []\n",
    "all_token_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_idx in tqdm(list_of_chunks_to_eval):\n",
    "    test_acts_chunk = dataset.get_test_acts(chunk_idx)\n",
    "    \n",
    "    # Store the row indices for this chunk\n",
    "    start_idx = chunk_idx * chunk_size\n",
    "    end_idx = start_idx + len(test_acts_chunk)\n",
    "    row_indices.extend(range(start_idx, end_idx))\n",
    "    \n",
    "    chunk_probe_outputs, _, token_scores = evaluate_probe_on_activation_dataset(\n",
    "        chunk_data=test_acts_chunk,\n",
    "        probe=probe,\n",
    "        device=None,\n",
    "        num_tokens=None,\n",
    "        verbose=False,\n",
    "        output_token_scores=True,\n",
    "    )\n",
    "    all_probe_outputs.extend(chunk_probe_outputs)\n",
    "    all_token_scores.extend(token_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = config['model_name']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_token_scores), len(chunk_probe_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_token_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.agent_logs_df.iloc[row_indices[33]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(words, color_array):\n",
    "    import plotly.colors as pc    \n",
    "    colorscale = [[0, 'rgb(255,0,0)'], [1, 'rgb(255,255,255)']]\n",
    "    template = '<span class=\"barcode\"; style=\"color: black; background-color: {}; font-size: 14px\">{}</span>'\n",
    "    colored_string = ''\n",
    "    for word, color in zip(words, color_array):\n",
    "        # Handle escape characters\n",
    "        if word == '\\n':\n",
    "            colored_string += '<br>'\n",
    "            continue\n",
    "        elif word == '\\n\\n':\n",
    "            colored_string += '<br><br>'\n",
    "            continue\n",
    "        elif word == '\\n\\n\\n':\n",
    "            colored_string += '<br><br><br>'\n",
    "            continue\n",
    "        elif word == '.\\n':\n",
    "            colored_string += '.<br>'\n",
    "            continue\n",
    "        elif word == '\\t':\n",
    "            colored_string += '&nbsp;&nbsp;&nbsp;&nbsp;'\n",
    "            continue\n",
    "        elif word == '\\\\' or word == '\\r':\n",
    "            # Skip other escape characters or handle them differently if needed\n",
    "            continue\n",
    "            \n",
    "        rgb_color = pc.sample_colorscale(colorscale, color)[0]\n",
    "        r, g, b = map(int, rgb_color.strip('rgb()').split(','))\n",
    "        hex_color = f'#{r:02x}{g:02x}{b:02x}'\n",
    "        colored_string += template.format(hex_color, '&nbsp' + word + '&nbsp')\n",
    "    \n",
    "    return colored_string\n",
    "\n",
    "def visualize_token_scores(row_index: int = 42):\n",
    "    row_token_scores = all_token_scores[row_index]\n",
    "    row_token_scores = np.array(row_token_scores)\n",
    "    raw_df = dataset.agent_logs_df\n",
    "    actual_row_idx = row_indices[row_index]\n",
    "\n",
    "    fp = dataset.agent_logs_row_to_full_prompt(\n",
    "        raw_df.iloc[actual_row_idx])\n",
    "    encoded = tokenizer.encode(fp, \n",
    "                              return_tensors=\"pt\")\n",
    "\n",
    "    full_prompt_tokens = []\n",
    "\n",
    "    for token_id in encoded[0]:\n",
    "        token = tokenizer.decode(token_id)\n",
    "        full_prompt_tokens.append(token)\n",
    "\n",
    "    words = full_prompt_tokens\n",
    "    color_array = row_token_scores\n",
    "\n",
    "    print(len(color_array))\n",
    "    return colorize(words, color_array), \\\n",
    "           all_token_scores[row_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the visualization to a file\n",
    "html_content = visualize_token_scores(42)[0]\n",
    "with open('plots/token_scores_42.html', 'w') as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "# Display in notebook\n",
    "HTML(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the visualization to a file\n",
    "html_content = visualize_token_scores(33)[0]\n",
    "with open('plots/token_scores_33.html', 'w') as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "# Display in notebook\n",
    "HTML(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_running_average_scores(scores1, scores2, window_size=None):\n",
    "    import plotly.graph_objects as go\n",
    "    import numpy as np\n",
    "    \n",
    "    # Calculate running averages\n",
    "    if window_size is None:\n",
    "        # Default behavior: cumulative running average\n",
    "        running_avg1 = np.cumsum(scores1) / np.arange(1, len(scores1) + 1)\n",
    "        running_avg2 = np.cumsum(scores2) / np.arange(1, len(scores2) + 1)\n",
    "    else:\n",
    "        # Windowed running average\n",
    "        running_avg1 = np.zeros(len(scores1))\n",
    "        running_avg2 = np.zeros(len(scores2))\n",
    "        \n",
    "        for i in range(len(scores1)):\n",
    "            start_idx = max(0, i + 1 - window_size)\n",
    "            window_len = i + 1 - start_idx\n",
    "            running_avg1[i] = np.sum(scores1[start_idx:i+1]) / window_len\n",
    "            \n",
    "        for i in range(len(scores2)):\n",
    "            start_idx = max(0, i + 1 - window_size)\n",
    "            window_len = i + 1 - start_idx\n",
    "            running_avg2[i] = np.sum(scores2[start_idx:i+1]) / window_len\n",
    "    \n",
    "    # Create x-axis values (token indices)\n",
    "    x_values = list(range(1, len(scores1) + 1))\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add traces for both running averages\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_values,\n",
    "        y=running_avg1,\n",
    "        mode='lines',\n",
    "        name='Crewmate',\n",
    "        line=dict(color='blue', width=3)  # Increased line thickness\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_values,\n",
    "        y=running_avg2,\n",
    "        mode='lines',\n",
    "        name='Impostor',\n",
    "        line=dict(color='red', width=3)  # Increased line thickness\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='',\n",
    "        xaxis_title='Token Index',\n",
    "        yaxis_title='Deception Probe (running avg.)',\n",
    "        template='plotly_white',\n",
    "        legend=dict(\n",
    "            x=0.62,\n",
    "            y=0.95,\n",
    "            # bgcolor='rgba(200, 200, 200, 0.8)',  # Darker background\n",
    "            bordercolor='rgba(0, 0, 0, 1)',      # Black border\n",
    "            borderwidth=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        template='plotly_white', \n",
    "        font=dict(\n",
    "            family=\"Computer Modern, LaTeX\", \n",
    "            size=15\n",
    "        ), \n",
    "        plot_bgcolor='#fafaf7'\n",
    "    )\n",
    "    \n",
    "    # Update all text elements to use LaTeX font and larger size\n",
    "    fig.update_layout(\n",
    "        title_font=dict(family=\"Computer Modern, LaTeX\", size=15),\n",
    "        xaxis=dict(\n",
    "            title_font=dict(family=\"Computer Modern, LaTeX\", size=15),\n",
    "            tickfont=dict(family=\"Computer Modern, LaTeX\", size=15)\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title_font=dict(family=\"Computer Modern, LaTeX\", size=15),\n",
    "            tickfont=dict(family=\"Computer Modern, LaTeX\", size=15)\n",
    "        ),\n",
    "        legend_font=dict(family=\"Computer Modern, LaTeX\", size=15),\n",
    "        width=600,\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "list1 = visualize_token_scores(42)[1]\n",
    "list2 = visualize_token_scores(33)[1]\n",
    "\n",
    "# Plot the running averages\n",
    "running_avg_plot = plot_running_average_scores(list1, list2, window_size=10000)\n",
    "running_avg_plot.show()\n",
    "\n",
    "# save the fig as a PDF (plotly)\n",
    "running_avg_plot.write_image(\"plots/probe_running_avg.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = visualize_token_scores(42)[1]\n",
    "list2 = visualize_token_scores(33)[1]\n",
    "\n",
    "# Plot the running averages\n",
    "running_avg_plot = plot_running_average_scores(list1, list2, window_size=100)\n",
    "running_avg_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amongus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}