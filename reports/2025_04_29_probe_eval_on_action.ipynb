{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'lolmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Linear Probes on the Model Output and Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch as t\n",
    "import importlib\n",
    "import os\n",
    "import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pandas import DataFrame, json_normalize\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../linear-probes/')\n",
    "from probes import LinearProbe\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "from evaluate_utils import evaluate_probe_on_activation_dataset\n",
    "from plots import plot_behavior_distribution, plot_roc_curves, add_roc_curves, print_metrics, plot_roc_curve_eval\n",
    "import probes\n",
    "from pprint import pprint as pp\n",
    "\n",
    "from probe_datasets import AmongUsDataset, TruthfulQADataset, DishonestQADataset, RepEngDataset, RolePlayingDataset, ApolloProbeDataset\n",
    "from configs import config_phi4, config_gpt2, config_llama3\n",
    "base_config = config_phi4\n",
    "amongus_expt_name: str = \"2025-02-01_phi_phi_100_games_v3\"\n",
    "# layers_to_work_on: List[int] = list(range(base_config[\"num_layers\"]))\n",
    "layers_to_work_on: List[int] = [20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import config_phi4, config_gpt2, config_llama3\n",
    "base_config = config_phi4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer: int = layers_to_work_on[0]\n",
    "probe_dataset_name: str = 'RepEngDataset'\n",
    "\n",
    "probe = LinearProbe(base_config[\"activation_size\"])\n",
    "        \n",
    "checkpoint_path = f'../linear-probes/checkpoints/{probe_dataset_name}_probe_{base_config[\"short_name\"]}_layer{layer}.pkl'\n",
    "\n",
    "with open(checkpoint_path, 'rb') as f:\n",
    "    probe.model = pickle.load(f).model\n",
    "    print(f'Loaded probe trained on {probe_dataset_name} for layer {layer}.')\n",
    "\n",
    "config = base_config.copy()\n",
    "config[\"layer\"] = str(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AmongUsDataset(config, model=None, tokenizer=None, device='cpu', expt_name=config['expt_name'], test_split=1)\n",
    "all_probe_outputs = []\n",
    "chunk_size: int = 500\n",
    "list_of_chunks_to_eval = [0, 1]\n",
    "row_indices = []\n",
    "all_token_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_idx in tqdm(list_of_chunks_to_eval):\n",
    "    test_acts_chunk = dataset.get_test_acts(chunk_idx)\n",
    "    \n",
    "    # Store the row indices for this chunk\n",
    "    start_idx = chunk_idx * chunk_size\n",
    "    end_idx = start_idx + len(test_acts_chunk)\n",
    "    row_indices.extend(range(start_idx, end_idx))\n",
    "    \n",
    "    chunk_probe_outputs, _, token_scores = evaluate_probe_on_activation_dataset(\n",
    "        chunk_data=test_acts_chunk,\n",
    "        probe=probe,\n",
    "        device=None,\n",
    "        num_tokens=None,\n",
    "        verbose=False,\n",
    "        output_token_scores=True,\n",
    "    )\n",
    "    all_probe_outputs.extend(chunk_probe_outputs)\n",
    "    all_token_scores.extend(token_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = config['model_name']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_token_scores), len(chunk_probe_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_token_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_token_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.agent_logs_df.iloc[row_indices[33]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = 0\n",
    "\n",
    "row_token_scores = all_token_scores[row_index]\n",
    "row_token_scores = np.array(row_token_scores)\n",
    "raw_df = dataset.agent_logs_df\n",
    "actual_row_idx = row_indices[row_index]\n",
    "\n",
    "fp = dataset.agent_logs_row_to_full_prompt(raw_df.iloc[actual_row_idx])\n",
    "encoded = tokenizer.encode(fp, return_tensors=\"pt\")\n",
    "\n",
    "full_prompt_tokens = []\n",
    "\n",
    "for token_id in encoded[0]:\n",
    "    token = tokenizer.decode(token_id)\n",
    "    full_prompt_tokens.append(token)\n",
    "\n",
    "words = full_prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index_words = []\n",
    "all_index_scores = []\n",
    "all_index_label = []\n",
    "all_index_action = []\n",
    "\n",
    "for row_index in tqdm(range(len(all_token_scores))):\n",
    "    row_token_scores = all_token_scores[row_index]\n",
    "    row_token_scores = np.array(row_token_scores)\n",
    "    actual_row_idx = row_indices[row_index]\n",
    "    raw_df = dataset.agent_logs_df\n",
    "    fp = dataset.agent_logs_row_to_full_prompt(raw_df.iloc[actual_row_idx])\n",
    "    identity = raw_df.iloc[actual_row_idx]['player.identity']\n",
    "    action = raw_df.iloc[actual_row_idx]['action']\n",
    "    encoded = tokenizer.encode(fp, return_tensors=\"pt\")\n",
    "\n",
    "    full_prompt_tokens = []\n",
    "\n",
    "    for token_id in encoded[0]:\n",
    "        token = tokenizer.decode(token_id)\n",
    "        full_prompt_tokens.append(token)\n",
    "\n",
    "    row_words = full_prompt_tokens\n",
    "    all_index_words.append(row_words)\n",
    "    all_index_scores.append(row_token_scores)\n",
    "    all_index_label.append(identity)\n",
    "    all_index_action.append(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'string_tokens': all_index_words, \n",
    "        'token_scores': all_index_scores, \n",
    "        'label': all_index_label,\n",
    "        'action': all_index_action\n",
    "    }\n",
    ")\n",
    "system_identifier = ['<|im_start|>', 'system', '<|im_sep|>']\n",
    "user_identifier = ['<|im_start|>', 'user', '<|im_sep|>']\n",
    "model_output_identifier = ['<|im_start|>', 'assistant', '<|im_sep|>']\n",
    "model_action_identifier = ['[', 'Action']\n",
    "model_action_identifier_2 = ['[', 'action']\n",
    "\n",
    "def find_sublist_start_idx(tokens, sublist, reversed=False):\n",
    "    n = len(tokens)\n",
    "    m = len(sublist)\n",
    "    \n",
    "    if reversed:\n",
    "        # Search from the end to find the last occurrence\n",
    "        for i in range(n - m, -1, -1):\n",
    "            if tokens[i:i+m] == sublist:\n",
    "                return i\n",
    "    else:\n",
    "        # Original behavior - find first occurrence\n",
    "        for i in range(n - m + 1):\n",
    "            if tokens[i:i+m] == sublist:\n",
    "                return i\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a soft mix parameter between mean and max aggregation\n",
    "k = 0  # 0 = pure mean, 1 = pure max\n",
    "\n",
    "df['system_start_idx'] = df['string_tokens'].apply(lambda tokens: find_sublist_start_idx(tokens, system_identifier))\n",
    "df['user_start_idx'] = df['string_tokens'].apply(lambda tokens: find_sublist_start_idx(tokens, user_identifier))\n",
    "df['model_output_start_idx'] = df['string_tokens'].apply(lambda tokens: find_sublist_start_idx(tokens, model_output_identifier))\n",
    "df['model_action_start_idx'] = df['string_tokens'].apply(lambda tokens: \\\n",
    "    max(find_sublist_start_idx(tokens, model_action_identifier, reversed=True), \\\n",
    "        find_sublist_start_idx(tokens, model_action_identifier_2, reversed=True)))\n",
    "\n",
    "# Define a function that creates a soft mix between mean and max\n",
    "def soft_agg(values):\n",
    "    if len(values) == 0:\n",
    "        return 0\n",
    "    return k * np.max(values) + (1-k) * np.mean(values)\n",
    "\n",
    "df['system_scores'] = df.apply(lambda row: soft_agg(row['token_scores'][row['system_start_idx']:row['user_start_idx']]), axis=1)\n",
    "df['user_scores'] = df.apply(lambda row: soft_agg(row['token_scores'][row['user_start_idx']:row['model_output_start_idx']]), axis=1)\n",
    "df['model_output_scores'] = df.apply(lambda row: soft_agg(row['token_scores'][row['model_output_start_idx']:]), axis=1)\n",
    "df['model_action_scores'] = df.apply(lambda row: soft_agg(row['token_scores'][row['model_action_start_idx']:]), axis=1)\n",
    "\n",
    "df['ground_truth'] = df['label'].apply(lambda x: 1 if x == 'Crewmate' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(df):\n",
    "    prediction_cols = ['system_scores', 'user_scores', 'model_output_scores', 'model_action_scores']\n",
    "    legend_names = ['+System', '+User', '+Thinking', 'Action']\n",
    "    \n",
    "    # Create figure with square shape\n",
    "    fig = go.Figure(layout=go.Layout(\n",
    "        width=500, \n",
    "        height=500,\n",
    "        font=dict(family=\"Computer Modern\", size=14),  # LaTeX-style font\n",
    "        template=\"plotly_white\"\n",
    "    ))\n",
    "    \n",
    "    # Color palette for elegant, deep colors\n",
    "    colors = ['#1f77b4', '#d62728', '#2ca02c', '#9467bd']\n",
    "    \n",
    "    # Add ROC curves with thicker lines and calculate AUROC\n",
    "    for i, col in enumerate(prediction_cols):\n",
    "        fpr, tpr, thresholds = roc_curve(df['ground_truth'], df[col])\n",
    "        auc_score = roc_auc_score(df['ground_truth'], df[col])\n",
    "        \n",
    "        # Format name with AUROC value\n",
    "        name = f\"{col} (AUC = {auc_score:.3f})\"\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=fpr, \n",
    "            y=tpr, \n",
    "            mode='lines', \n",
    "            name=f'{legend_names[i]} (AUC = {auc_score:.3f})',\n",
    "            line=dict(color=colors[i], width=3),\n",
    "        ))\n",
    "    \n",
    "    # Add random baseline (y=x) as dashed black line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[0, 1], \n",
    "        y=[0, 1], \n",
    "        mode='lines', \n",
    "        name='Random (AUC = 0.500)',\n",
    "        line=dict(color='black', width=2, dash='dash'),\n",
    "    ))\n",
    "    \n",
    "    # Update layout for research paper quality\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            title='False Positive Rate',\n",
    "            titlefont=dict(size=16),\n",
    "            tickfont=dict(size=14),\n",
    "            gridcolor='#E9E9E9',\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='True Positive Rate',\n",
    "            titlefont=dict(size=16),\n",
    "            tickfont=dict(size=14),\n",
    "            gridcolor='#E9E9E9',\n",
    "        ),\n",
    "        legend=dict(\n",
    "            x=0.45,\n",
    "            y=0.1,\n",
    "            bgcolor='rgba(255, 255, 255, 0.8)',\n",
    "            bordercolor='#E2E2E2',\n",
    "            borderwidth=1,\n",
    "            traceorder='reversed',  # Reverse the order of the legend items\n",
    "        ),\n",
    "        plot_bgcolor='white',\n",
    "        margin=dict(l=60, r=30, t=30, b=60),\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(width=520, height=500)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_roc_curves(df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as a high quality pdf\n",
    "fig.write_image(\"plots/output_action_roc_mean.pdf\", width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter on SPEAK Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speak = df[df['action'].str.contains('SPEAK')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_roc_curves(df_speak)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"plots/output_action_roc_mean_speak.pdf\", width=500, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train probes on just action / speech actions and thinking + actions (model output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amongus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}